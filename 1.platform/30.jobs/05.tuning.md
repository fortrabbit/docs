---
reviewed: 2025-07-17 08:05:18
title: Jobs tuning
naviTitle: Tuning
navigation.excerpt: Extended knowledge on jobs
---

## Logging

Both STDOUT and STDERR generated by any job are logged by our platform, but aren't accessible right now. They will eventually be shown in the dashboard.

## Restart after code update

This happens automatically. Whenever you push a new code update via [git deployment](/1.platform/05.deployment/01.intro.md), jobs will be shutdown and started anew.

## Considerations

- The total (max) memory amount of all jobs should be below the memory limit of the plan.
- Exits of jobs should be looked into. Usually nonstop worker jobs should run continuously and not exit often.

The memory limit is for all jobs combined. Resources needed for each application can vary largely, depending on what each particular job is doing.

## Graceful shutdown

If your job does really long running processing, it's likely running at all times. Any code push or manual deployment forces a restart of all jobs.

To avoid losing valuable state when our system kills the job, you can use Unix signal handling to write a graceful shutdown handler. For this you can use the automatically available [PCNTL](http://php.net/manual/en/book.pcntl.php) extension.

The most simplistic PHP script for a job is a while loop:

```php
while (true) {
  do_something();
  sleep(5);
}
```

To make sure that `do_something()` is never aborted, you can extend the script like so:

```php
declare(ticks=1);

$shutdown = false;
pcntl_signal(SIGTERM, function($signo) use (&$shutdown) {
    error_log("Received shutdown signal");
    $shutdown = true;
});

while (true) {
  do_something();
  foreach (range(1, 5) as $num) {
    if ($shutdown) {
      error_log("Shutting down safely");
      exit(0);
    }
    sleep(1);
  }
}
```

## Do not detach

If you don't know what that is: never mind. If you do know: don't detach. To guarantee that we can monitor jobs correctly they need to run with-under the parent processes which started them. All detached processes will be killed.

## Overlapping jobs

Sometimes a cron job may be called quickly multiple times and thus end up with multiple instances of the same job running. At best you should design your application so that this does not happen or happens in a controlled manner. We have reasonable limits in place to control how many instances of the same job are allowed to run in parallel.
